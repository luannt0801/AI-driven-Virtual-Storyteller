{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac1dac95",
   "metadata": {},
   "source": [
    "## Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec280f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\2025\\Master BKHN\\Ky thuat lap trinh noi dung so\\AI-driven-Virtual-Storyteller\\luan\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "path_model = \"D:\\\\2025\\\\Master BKHN\\\\Ky thuat lap trinh noi dung so\\\\AI-driven-Virtual-Storyteller\\\\models\"\n",
    "\n",
    "os.environ[\"TRANSFORMER_CACHE\"] = path_model\n",
    "\n",
    "model_name = \"openai-community/gpt2\"\n",
    "# model_name = \"openai/gpt-oss-20b\"\n",
    "\n",
    "# Load model & tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=path_model \n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45990f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"hello, tell me a story about money.\\n\\nL.C.\\n\\nNo one should lose money with this stuff. Even your granddad, if you're going to do this.\\n\\nI'm not kidding when I say you can do a better job of doing this.\\n\\nI don't know what these people thought of you when you were writing to them.\\n\\nSo I don't really care. I'd love to, if only we could be friends like this!\\n\\nYou can call. I'll\"]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"hello, tell me a story about money\"\n",
    "input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(input.input_ids, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n",
    "\n",
    "output_string = tokenizer.batch_decode(outputs)\n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a09a8",
   "metadata": {},
   "source": [
    "## Handle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36de4481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "short_stories_dataset = load_dataset(\"roneneldan/TinyStories\",\n",
    "                                     cache_dir=\"D:\\\\2025\\\\Master BKHN\\\\Ky thuat lap trinh noi dung so\\\\AI-driven-Virtual-Storyteller\\\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecf1cf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length train: 2119719\n",
      "length test: 21990\n"
     ]
    }
   ],
   "source": [
    "print(f\"length train: {len(short_stories_dataset[\"train\"])}\")\n",
    "print(f\"length test: {len(short_stories_dataset[\"validation\"])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fad0d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with small story dataset\n",
    "small_story_dataset = load_dataset(\n",
    "    \"roneneldan/TinyStories\",\n",
    "    cache_dir=\"D:\\\\2025\\\\Master BKHN\\\\Ky thuat lap trinh noi dung so\\\\AI-driven-Virtual-Storyteller\\\\data\",\n",
    "    split=\"train[:1000]\")\n",
    "\n",
    "small_story_dataset_train = small_story_dataset.train_test_split(train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e25e74cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[105,\n",
       " 174,\n",
       " 277,\n",
       " 150,\n",
       " 147,\n",
       " 117,\n",
       " 158,\n",
       " 418,\n",
       " 168,\n",
       " 145,\n",
       " 296,\n",
       " 117,\n",
       " 192,\n",
       " 166,\n",
       " 176,\n",
       " 93,\n",
       " 198,\n",
       " 138,\n",
       " 175,\n",
       " 138,\n",
       " 102,\n",
       " 120,\n",
       " 176,\n",
       " 156,\n",
       " 137,\n",
       " 319,\n",
       " 94,\n",
       " 148,\n",
       " 144,\n",
       " 231,\n",
       " 276,\n",
       " 192,\n",
       " 165,\n",
       " 270,\n",
       " 121,\n",
       " 189,\n",
       " 84,\n",
       " 207,\n",
       " 208,\n",
       " 521,\n",
       " 184,\n",
       " 114,\n",
       " 129,\n",
       " 128,\n",
       " 163,\n",
       " 72,\n",
       " 276,\n",
       " 196,\n",
       " 187,\n",
       " 114,\n",
       " 202,\n",
       " 102,\n",
       " 278,\n",
       " 141,\n",
       " 151,\n",
       " 117,\n",
       " 370,\n",
       " 184,\n",
       " 138,\n",
       " 163,\n",
       " 165,\n",
       " 441,\n",
       " 197,\n",
       " 232,\n",
       " 163,\n",
       " 153,\n",
       " 151,\n",
       " 112,\n",
       " 190,\n",
       " 442,\n",
       " 137,\n",
       " 164,\n",
       " 165,\n",
       " 310,\n",
       " 278,\n",
       " 136,\n",
       " 129,\n",
       " 294,\n",
       " 146,\n",
       " 154,\n",
       " 125,\n",
       " 272,\n",
       " 110,\n",
       " 217,\n",
       " 141,\n",
       " 374,\n",
       " 133,\n",
       " 137,\n",
       " 129,\n",
       " 97,\n",
       " 160,\n",
       " 136,\n",
       " 110,\n",
       " 157,\n",
       " 138,\n",
       " 148,\n",
       " 101,\n",
       " 143,\n",
       " 165,\n",
       " 217,\n",
       " 158,\n",
       " 300,\n",
       " 129,\n",
       " 188,\n",
       " 126,\n",
       " 163,\n",
       " 95,\n",
       " 147,\n",
       " 175,\n",
       " 323,\n",
       " 147,\n",
       " 116,\n",
       " 174,\n",
       " 145,\n",
       " 219,\n",
       " 130,\n",
       " 107,\n",
       " 364,\n",
       " 143,\n",
       " 94,\n",
       " 157,\n",
       " 203,\n",
       " 157,\n",
       " 119,\n",
       " 194,\n",
       " 138,\n",
       " 159,\n",
       " 130,\n",
       " 176,\n",
       " 151,\n",
       " 596,\n",
       " 150,\n",
       " 156,\n",
       " 196,\n",
       " 137,\n",
       " 208,\n",
       " 138,\n",
       " 172,\n",
       " 317,\n",
       " 120,\n",
       " 202,\n",
       " 110,\n",
       " 150,\n",
       " 496,\n",
       " 181,\n",
       " 110,\n",
       " 155,\n",
       " 127,\n",
       " 120,\n",
       " 97,\n",
       " 183,\n",
       " 129,\n",
       " 170,\n",
       " 166,\n",
       " 167,\n",
       " 161,\n",
       " 360,\n",
       " 215,\n",
       " 164,\n",
       " 137,\n",
       " 105,\n",
       " 151,\n",
       " 267,\n",
       " 116,\n",
       " 216,\n",
       " 119,\n",
       " 148,\n",
       " 180,\n",
       " 137,\n",
       " 82,\n",
       " 129,\n",
       " 152,\n",
       " 247,\n",
       " 163,\n",
       " 124,\n",
       " 151,\n",
       " 159,\n",
       " 160,\n",
       " 141,\n",
       " 187,\n",
       " 84,\n",
       " 158,\n",
       " 187,\n",
       " 139,\n",
       " 167,\n",
       " 236,\n",
       " 268,\n",
       " 136,\n",
       " 124,\n",
       " 137,\n",
       " 123,\n",
       " 155,\n",
       " 168,\n",
       " 174,\n",
       " 204,\n",
       " 139,\n",
       " 117,\n",
       " 342,\n",
       " 162,\n",
       " 179,\n",
       " 129,\n",
       " 145,\n",
       " 139,\n",
       " 149,\n",
       " 139,\n",
       " 148,\n",
       " 123,\n",
       " 205,\n",
       " 140,\n",
       " 101,\n",
       " 137,\n",
       " 177,\n",
       " 159,\n",
       " 174,\n",
       " 140,\n",
       " 194,\n",
       " 143,\n",
       " 433,\n",
       " 106,\n",
       " 152,\n",
       " 218,\n",
       " 219,\n",
       " 151,\n",
       " 132,\n",
       " 322,\n",
       " 151,\n",
       " 118,\n",
       " 149,\n",
       " 106,\n",
       " 174,\n",
       " 175,\n",
       " 106,\n",
       " 177,\n",
       " 150,\n",
       " 157,\n",
       " 142,\n",
       " 257,\n",
       " 262,\n",
       " 122,\n",
       " 323,\n",
       " 120,\n",
       " 109,\n",
       " 145,\n",
       " 462,\n",
       " 203,\n",
       " 184,\n",
       " 160,\n",
       " 173,\n",
       " 203,\n",
       " 129,\n",
       " 147,\n",
       " 173,\n",
       " 147,\n",
       " 177,\n",
       " 163,\n",
       " 146,\n",
       " 122,\n",
       " 401,\n",
       " 483,\n",
       " 136,\n",
       " 158,\n",
       " 107,\n",
       " 144,\n",
       " 189,\n",
       " 138,\n",
       " 220,\n",
       " 233,\n",
       " 240,\n",
       " 128,\n",
       " 296,\n",
       " 137,\n",
       " 265,\n",
       " 158,\n",
       " 305,\n",
       " 131,\n",
       " 141,\n",
       " 151,\n",
       " 156,\n",
       " 116,\n",
       " 202,\n",
       " 142,\n",
       " 151,\n",
       " 145,\n",
       " 148,\n",
       " 120,\n",
       " 127,\n",
       " 128,\n",
       " 105,\n",
       " 349,\n",
       " 298,\n",
       " 134,\n",
       " 150,\n",
       " 112,\n",
       " 121,\n",
       " 151,\n",
       " 385,\n",
       " 318,\n",
       " 151,\n",
       " 160,\n",
       " 170,\n",
       " 220,\n",
       " 103,\n",
       " 253,\n",
       " 209,\n",
       " 139,\n",
       " 157,\n",
       " 305,\n",
       " 115,\n",
       " 150,\n",
       " 119,\n",
       " 157,\n",
       " 97,\n",
       " 164,\n",
       " 252,\n",
       " 172,\n",
       " 142,\n",
       " 156,\n",
       " 215,\n",
       " 343,\n",
       " 148,\n",
       " 167,\n",
       " 107,\n",
       " 411,\n",
       " 178,\n",
       " 163,\n",
       " 167,\n",
       " 145,\n",
       " 141,\n",
       " 129,\n",
       " 183,\n",
       " 143,\n",
       " 117,\n",
       " 119,\n",
       " 90,\n",
       " 141,\n",
       " 144,\n",
       " 138,\n",
       " 188,\n",
       " 194,\n",
       " 132,\n",
       " 171,\n",
       " 212,\n",
       " 149,\n",
       " 86,\n",
       " 189,\n",
       " 171,\n",
       " 104,\n",
       " 289,\n",
       " 188,\n",
       " 140,\n",
       " 161,\n",
       " 95,\n",
       " 195,\n",
       " 140,\n",
       " 161,\n",
       " 128,\n",
       " 397,\n",
       " 164,\n",
       " 243,\n",
       " 132,\n",
       " 127,\n",
       " 116,\n",
       " 150,\n",
       " 144,\n",
       " 159,\n",
       " 137,\n",
       " 207,\n",
       " 112,\n",
       " 144,\n",
       " 160,\n",
       " 154,\n",
       " 86,\n",
       " 176,\n",
       " 155,\n",
       " 110,\n",
       " 184,\n",
       " 145,\n",
       " 144,\n",
       " 290,\n",
       " 190,\n",
       " 127,\n",
       " 118,\n",
       " 215,\n",
       " 141,\n",
       " 193,\n",
       " 477,\n",
       " 165,\n",
       " 116,\n",
       " 154,\n",
       " 338,\n",
       " 141,\n",
       " 152,\n",
       " 391,\n",
       " 142,\n",
       " 179,\n",
       " 177,\n",
       " 141,\n",
       " 118,\n",
       " 142,\n",
       " 188,\n",
       " 224,\n",
       " 344,\n",
       " 161,\n",
       " 152,\n",
       " 145,\n",
       " 142,\n",
       " 125,\n",
       " 131,\n",
       " 135,\n",
       " 106,\n",
       " 137,\n",
       " 189,\n",
       " 150,\n",
       " 141,\n",
       " 125,\n",
       " 140,\n",
       " 147,\n",
       " 91,\n",
       " 164,\n",
       " 133,\n",
       " 114,\n",
       " 155,\n",
       " 166,\n",
       " 183,\n",
       " 147,\n",
       " 232,\n",
       " 142,\n",
       " 130,\n",
       " 397,\n",
       " 126,\n",
       " 142,\n",
       " 116,\n",
       " 153,\n",
       " 108,\n",
       " 160,\n",
       " 135,\n",
       " 107,\n",
       " 170,\n",
       " 114,\n",
       " 161,\n",
       " 158,\n",
       " 133,\n",
       " 155,\n",
       " 66,\n",
       " 110,\n",
       " 262,\n",
       " 194,\n",
       " 206,\n",
       " 325,\n",
       " 123,\n",
       " 172,\n",
       " 142,\n",
       " 128,\n",
       " 360,\n",
       " 124,\n",
       " 154,\n",
       " 165,\n",
       " 155,\n",
       " 206,\n",
       " 148,\n",
       " 196,\n",
       " 212,\n",
       " 148,\n",
       " 140,\n",
       " 111,\n",
       " 118,\n",
       " 205,\n",
       " 157,\n",
       " 201,\n",
       " 132,\n",
       " 168,\n",
       " 104,\n",
       " 152,\n",
       " 158,\n",
       " 318,\n",
       " 176,\n",
       " 150,\n",
       " 159,\n",
       " 198,\n",
       " 114,\n",
       " 143,\n",
       " 127,\n",
       " 170,\n",
       " 172,\n",
       " 171,\n",
       " 167,\n",
       " 137,\n",
       " 138,\n",
       " 136,\n",
       " 194,\n",
       " 90,\n",
       " 221,\n",
       " 176,\n",
       " 146,\n",
       " 204,\n",
       " 146,\n",
       " 142,\n",
       " 143,\n",
       " 196,\n",
       " 86,\n",
       " 101,\n",
       " 103,\n",
       " 116,\n",
       " 317,\n",
       " 177,\n",
       " 191,\n",
       " 125,\n",
       " 583,\n",
       " 184,\n",
       " 151,\n",
       " 132,\n",
       " 157,\n",
       " 211,\n",
       " 160,\n",
       " 162,\n",
       " 258,\n",
       " 80,\n",
       " 140,\n",
       " 171,\n",
       " 159,\n",
       " 150,\n",
       " 164,\n",
       " 203,\n",
       " 167,\n",
       " 395,\n",
       " 118,\n",
       " 109,\n",
       " 125,\n",
       " 413,\n",
       " 155,\n",
       " 179,\n",
       " 132,\n",
       " 118,\n",
       " 315,\n",
       " 281,\n",
       " 106,\n",
       " 132,\n",
       " 108,\n",
       " 201,\n",
       " 179,\n",
       " 119,\n",
       " 162,\n",
       " 109,\n",
       " 144,\n",
       " 151,\n",
       " 272,\n",
       " 102,\n",
       " 144,\n",
       " 185,\n",
       " 228,\n",
       " 188,\n",
       " 155,\n",
       " 115,\n",
       " 117,\n",
       " 170,\n",
       " 110,\n",
       " 165,\n",
       " 150,\n",
       " 164,\n",
       " 107,\n",
       " 236,\n",
       " 171,\n",
       " 367,\n",
       " 236,\n",
       " 202,\n",
       " 141,\n",
       " 179,\n",
       " 125,\n",
       " 309,\n",
       " 164,\n",
       " 598,\n",
       " 112,\n",
       " 107,\n",
       " 130,\n",
       " 183,\n",
       " 134,\n",
       " 158,\n",
       " 155,\n",
       " 453,\n",
       " 138,\n",
       " 116,\n",
       " 397,\n",
       " 141,\n",
       " 368,\n",
       " 124,\n",
       " 155,\n",
       " 330,\n",
       " 99,\n",
       " 272,\n",
       " 134,\n",
       " 177,\n",
       " 141,\n",
       " 299,\n",
       " 120,\n",
       " 113,\n",
       " 263,\n",
       " 144,\n",
       " 140,\n",
       " 199,\n",
       " 171,\n",
       " 200,\n",
       " 435,\n",
       " 157,\n",
       " 122,\n",
       " 128,\n",
       " 144,\n",
       " 137,\n",
       " 149,\n",
       " 173,\n",
       " 354,\n",
       " 170,\n",
       " 285,\n",
       " 162,\n",
       " 159,\n",
       " 169,\n",
       " 164,\n",
       " 298,\n",
       " 320,\n",
       " 407,\n",
       " 136,\n",
       " 462,\n",
       " 168,\n",
       " 163,\n",
       " 148,\n",
       " 112,\n",
       " 133,\n",
       " 125,\n",
       " 112,\n",
       " 177,\n",
       " 522,\n",
       " 179,\n",
       " 154,\n",
       " 121,\n",
       " 288,\n",
       " 276,\n",
       " 304,\n",
       " 271,\n",
       " 131,\n",
       " 183,\n",
       " 190,\n",
       " 161,\n",
       " 149,\n",
       " 124,\n",
       " 105,\n",
       " 137,\n",
       " 188,\n",
       " 447,\n",
       " 192,\n",
       " 200,\n",
       " 162,\n",
       " 111,\n",
       " 145,\n",
       " 104,\n",
       " 153,\n",
       " 167,\n",
       " 155,\n",
       " 145,\n",
       " 349,\n",
       " 300,\n",
       " 130,\n",
       " 184,\n",
       " 138,\n",
       " 99,\n",
       " 198,\n",
       " 158,\n",
       " 135,\n",
       " 285,\n",
       " 139,\n",
       " 164,\n",
       " 117,\n",
       " 123,\n",
       " 94,\n",
       " 138,\n",
       " 478,\n",
       " 61,\n",
       " 175,\n",
       " 193,\n",
       " 94,\n",
       " 821,\n",
       " 112,\n",
       " 381,\n",
       " 107,\n",
       " 157,\n",
       " 333,\n",
       " 182,\n",
       " 106,\n",
       " 155,\n",
       " 365,\n",
       " 240,\n",
       " 257,\n",
       " 137,\n",
       " 153,\n",
       " 133,\n",
       " 111,\n",
       " 387,\n",
       " 188,\n",
       " 154,\n",
       " 123,\n",
       " 299,\n",
       " 197,\n",
       " 174,\n",
       " 146,\n",
       " 189,\n",
       " 147,\n",
       " 130,\n",
       " 218,\n",
       " 151,\n",
       " 104,\n",
       " 154,\n",
       " 203,\n",
       " 182,\n",
       " 263,\n",
       " 135,\n",
       " 142,\n",
       " 141,\n",
       " 227,\n",
       " 135,\n",
       " 179,\n",
       " 161,\n",
       " 134,\n",
       " 333,\n",
       " 198,\n",
       " 135,\n",
       " 151,\n",
       " 182,\n",
       " 311,\n",
       " 144,\n",
       " 124,\n",
       " 140,\n",
       " 149,\n",
       " 411,\n",
       " 244,\n",
       " 109,\n",
       " 309,\n",
       " 133,\n",
       " 141,\n",
       " 177,\n",
       " 143,\n",
       " 193,\n",
       " 130,\n",
       " 255,\n",
       " 141,\n",
       " 276,\n",
       " 161,\n",
       " 168,\n",
       " 156,\n",
       " 139,\n",
       " 232,\n",
       " 142,\n",
       " 132,\n",
       " 291,\n",
       " 89,\n",
       " 470,\n",
       " 167,\n",
       " 137,\n",
       " 109,\n",
       " 208,\n",
       " 187,\n",
       " 226,\n",
       " 166,\n",
       " 126,\n",
       " 220,\n",
       " 128,\n",
       " 134,\n",
       " 138,\n",
       " 140,\n",
       " 154,\n",
       " 151,\n",
       " 328,\n",
       " 150,\n",
       " 168,\n",
       " 166,\n",
       " 183,\n",
       " 198,\n",
       " 156,\n",
       " 157,\n",
       " 164,\n",
       " 127,\n",
       " 147,\n",
       " 181,\n",
       " 215,\n",
       " 181,\n",
       " 235,\n",
       " 203,\n",
       " 122,\n",
       " 328,\n",
       " 150,\n",
       " 137,\n",
       " 170,\n",
       " 139,\n",
       " 280,\n",
       " 152,\n",
       " 307,\n",
       " 365,\n",
       " 175,\n",
       " 144,\n",
       " 113,\n",
       " 140,\n",
       " 198,\n",
       " 414]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x[\"text\"].split(\" \")) for x in small_story_dataset_train[\"train\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1821d7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 800/800 [00:00<00:00, 3611.95 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 3155.07 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize dataset\n",
    "\n",
    "def preprocess_batch(batch):\n",
    "    all_text_times = batch[\"text\"]\n",
    "    trimmed_text_times = [x[:500] for x in all_text_times]\n",
    "    return tokenizer(trimmed_text_times)\n",
    "\n",
    "tokenized_dataset = small_story_dataset_train.map(\n",
    "    preprocess_batch,\n",
    "    batched=True,\n",
    "    batch_size=10,\n",
    "    remove_columns=small_story_dataset_train[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88a95d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       "), mlm=False, mlm_probability=0.15, mask_replace_prob=0.8, random_replace_prob=0.1, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt', seed=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4da66ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 03:55, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.127047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.061836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.033153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.018727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.037600</td>\n",
       "      <td>2.008997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.037600</td>\n",
       "      <td>2.001841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.037600</td>\n",
       "      <td>1.998406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.037600</td>\n",
       "      <td>1.996774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.037600</td>\n",
       "      <td>1.996893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.789100</td>\n",
       "      <td>1.997239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=1.9133826293945313, metrics={'train_runtime': 236.2687, 'train_samples_per_second': 33.86, 'train_steps_per_second': 4.232, 'total_flos': 573470687232000.0, 'train_loss': 1.9133826293945313, 'epoch': 10.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    args=training_args,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7523aa4",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15b17f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_infer = AutoModelForCausalLM.from_pretrained(\"./output/checkpoint-1000/\")\n",
    "model_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e449196a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Once and a time there was a young boy named Timmy. Timmy loved to play with his toys and was always eager to get something new. One day, Timmy went to the park and got to play with his toys. He quickly picked up some shiny blocks and started to play with them. Soon enough, he found himself getting more and more excited. \\n\\nTimmy's Mommy explained to Timmy how important it was to have a healthy snack every day. Timmy was so\"]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once\"\n",
    "input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model_infer.generate(input.input_ids, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n",
    "\n",
    "output_string = tokenizer.batch_decode(outputs)\n",
    "print(output_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "luan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
